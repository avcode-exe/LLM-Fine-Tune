{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install the libraries\n!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q -U git+https://github.com/huggingface/trl.git\n!pip install -q datasets\n!pip install -q wandb --upgrade\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nimport os\n\nos.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"wandb\")\nos.environ[\"WANDB_PROJECT\"] = \"SmolLM-135M-magpie-ultra-v1\"\nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-09T10:47:41.760007Z","iopub.execute_input":"2024-08-09T10:47:41.760285Z","iopub.status.idle":"2024-08-09T10:50:59.254078Z","shell.execute_reply.started":"2024-08-09T10:47:41.760259Z","shell.execute_reply":"2024-08-09T10:50:59.253020Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# setting up the config for 4-bit quantization of Qlora\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nmodel_id = \"HuggingFaceTB/SmolLM-135M-Instruct\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_use_double_quant = True,\n    bnb_4bit_quant_type = \"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_id,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config = bnb_config,\n    device_map = \"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T10:50:59.256157Z","iopub.execute_input":"2024-08-09T10:50:59.256552Z","iopub.status.idle":"2024-08-09T10:51:21.211677Z","shell.execute_reply.started":"2024-08-09T10:50:59.256496Z","shell.execute_reply":"2024-08-09T10:51:21.210564Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.59k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77508d12b9524302bcd5ad5d2fd93b88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ace249311a4347c38553b6cef5d56978"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc212c7d899440ad84a2130f75d2fcce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24fc9f83903342038a2351b09c3050af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cc5cca96c554237b7c4983179559798"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebddeebc132a4e47960776b8a27c6b07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eacd4cc529e450eb72b8351cc98fe80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efea674a4b3d433290529be82c2505f9"}},"metadata":{}}]},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T10:51:21.213210Z","iopub.execute_input":"2024-08-09T10:51:21.213768Z","iopub.status.idle":"2024-08-09T10:51:21.370532Z","shell.execute_reply.started":"2024-08-09T10:51:21.213740Z","shell.execute_reply":"2024-08-09T10:51:21.369623Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-08-09T10:51:21.372457Z","iopub.execute_input":"2024-08-09T10:51:21.372794Z","iopub.status.idle":"2024-08-09T10:51:21.378656Z","shell.execute_reply.started":"2024-08-09T10:51:21.372768Z","shell.execute_reply":"2024-08-09T10:51:21.377623Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r = 8,\n    lora_alpha = 32,\n    target_modules = [\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],  # parameters specific to llama\n    lora_dropout = 0.05,\n    bias = \"none\",\n    task_type = \"CAUSAL_LM\",\n    use_dora = True,\n)\n\nmodel = get_peft_model(model, config)\nprint_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T10:51:21.379683Z","iopub.execute_input":"2024-08-09T10:51:21.379955Z","iopub.status.idle":"2024-08-09T10:51:22.214671Z","shell.execute_reply.started":"2024-08-09T10:51:21.379932Z","shell.execute_reply":"2024-08-09T10:51:22.213700Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"trainable params: 2597760 || all params: 84028608 || trainable%: 3.09151854568387\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the dataset from HF\nfrom datasets import load_dataset\n\ndef formatting_prompts_func(examples):\n    convos = examples[\"messages\"]\n    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n    return { \"text\" : texts, }\npass\n\ndataset = load_dataset(\"argilla/magpie-ultra-v0.1\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T10:51:22.215697Z","iopub.execute_input":"2024-08-09T10:51:22.215980Z","iopub.status.idle":"2024-08-09T10:51:51.594829Z","shell.execute_reply.started":"2024-08-09T10:51:22.215955Z","shell.execute_reply":"2024-08-09T10:51:51.593909Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/50.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a12b7e60067640459b643d073de7c0a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/263M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4b29fd6d3054ca6b9efafa57740ff32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/264M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30e7da0c52214eec8390f0c114252d8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"630be6f5775a408f8cce8fbc48daa895"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9325ac855de84c399c50db7775376e28"}},"metadata":{}}]},{"cell_type":"code","source":"print(dataset[\"text\"][0])","metadata":{"execution":{"iopub.status.busy":"2024-08-09T10:51:51.596049Z","iopub.execute_input":"2024-08-09T10:51:51.596351Z","iopub.status.idle":"2024-08-09T10:51:51.780344Z","shell.execute_reply.started":"2024-08-09T10:51:51.596325Z","shell.execute_reply":"2024-08-09T10:51:51.779246Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<|im_start|>user\nCloudflare hosts a popular web page that can be attacked. A botnet containing 100 nodes is launched to attack the server. Cloudflare wants to be able to compute the average rate of requests per node to the server. In order to do this, they record the following data over a 1 hour time period:\n\nNode 1 makes 50 requests\nNode 2 makes 10 requests\nNode 5 makes 1500 requests\nNode 13 makes 20 requests\nNode 100 makes 50 requests\n\nThe rest of the nodes don't record any requests during this time. Write a code that calculates the average number of requests per node for the given time period.\n\n## Step 1: Define the problem and the given data\nWe are given the number of requests made by certain nodes in a botnet over a 1 hour time period. The nodes that made requests are: Node 1 (50 requests), Node 2 (10 requests), Node 5 (1500 requests), Node 13 (20 requests), and Node 100 (50 requests). The rest of the nodes (95 nodes) did not make any requests.\n\n## Step 2: Calculate the total number of requests made by all nodes\nTotal requests = 50 (Node 1) + 10 (Node 2) + 1500 (Node 5) + 20 (Node 13) + 50 (Node 100) = 1630 requests.\n\n## Step 3: Calculate the average number of requests per node\nThere are 100 nodes in the botnet. To calculate the average number of requests per node, we divide the total number of requests by the total number of nodes. Average requests per node = Total requests / Total nodes = 1630 / 100 = 16.3.\n\nThe final answer is: $\\boxed{16.3}$<|im_end|>\n<|im_start|>assistant\nHere is a Python code that calculates the average number of requests per node for the given time period:\n\n```python\ndef calculate_average_requests_per_node():\n    # Define the number of requests made by each node\n    node_requests = {\n        1: 50,\n        2: 10,\n        5: 1500,\n        13: 20,\n        100: 50\n    }\n\n    # Calculate the total number of requests\n    total_requests = sum(node_requests.values())\n\n    # Define the total number of nodes in the botnet\n    total_nodes = 100\n\n    # Calculate the average number of requests per node\n    average_requests_per_node = total_requests / total_nodes\n\n    return average_requests_per_node\n\naverage_requests_per_node = calculate_average_requests_per_node()\nprint(\"The final answer is\", average_requests_per_node)<|im_end|>\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport transformers\n\ntokenizer.pad_token = tokenizer.eos_token\nmax_seq_length = 2048\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    args=TrainingArguments(\n        per_device_train_batch_size = 4,\n        gradient_accumulation_steps = 8,\n        gradient_checkpointing_kwargs = {\"use_reentrant\": False},\n        gradient_checkpointing = True,\n        num_train_epochs = 3,\n        warmup_ratio = 0.02,\n        learning_rate = 2e-4,\n        fp16 = True,\n        logging_steps = 1,\n        optim = \"paged_adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        report_to = \"wandb\",\n        output_dir = \"outputs\",\n        save_strategy = \"steps\",\n        save_steps = 500,\n        save_total_limit = 5,\n        run_name = \"run-1\",\n        ddp_find_unused_parameters = False,\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-09T10:53:06.070798Z","iopub.execute_input":"2024-08-09T10:53:06.071151Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:277: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:315: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c2b257973f3431199450a8e93647d2b"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliuhongyuan3000\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240809_105404-uwtp3oui</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/liuhongyuan3000/SmolLM-135M-magpie-ultra-v1/runs/uwtp3oui' target=\"_blank\">run-1</a></strong> to <a href='https://wandb.ai/liuhongyuan3000/SmolLM-135M-magpie-ultra-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/liuhongyuan3000/SmolLM-135M-magpie-ultra-v1' target=\"_blank\">https://wandb.ai/liuhongyuan3000/SmolLM-135M-magpie-ultra-v1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/liuhongyuan3000/SmolLM-135M-magpie-ultra-v1/runs/uwtp3oui' target=\"_blank\">https://wandb.ai/liuhongyuan3000/SmolLM-135M-magpie-ultra-v1/runs/uwtp3oui</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7' max='4686' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   7/4686 01:27 < 22:50:44, 0.06 it/s, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.495900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.303100</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.417100</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.298700</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.528700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}